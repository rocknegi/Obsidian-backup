-  [Why does the overfitting decreases if we choose K to be large in K-nearest neighbors?](https://datascience.stackexchange.com/questions/81866/why-does-the-overfitting-decreases-if-we-choose-k-to-be-large-in-k-nearest-neigh)

  
- What is Regularization in Machine Learning? Regularization refers to **techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting**.

- What is the difference between parameter and hyperparameter?

	- Model Parameters: These are the parameters in the model that must be determined using the training data set. These are the fitted parameters. **Hyperparameters: These are adjustable parameters that must be tuned in order to obtain a model with optimal performance**

-  What Is Grid Search?
	- Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on a the specific parameter values of a model. The model is also known as an estimator.Grid search exercise can save us time, effort and resources. 
-  Definite matrix 
	- https://en.wikipedia.org/wiki/Definite_matrix  
- graph of xˆ3
  -	![[Pasted image 20220712161951.png]]
-  C = 0 and C = infinite
	- **C hyperparameter** adds a penalty for each misclassified data point.
	- Large Value of parameter C implies a small margin, there is a tendency to overfit the training model.
	-  Small Value of parameter C implies a large margin which might lead to underfitting of the model.
	- [machine learning - What is the influence of C in SVMs with linear kernel? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel)

  

**Dimensionality reduction (DR) is another useful technique that can be used to mitigate/**less harsh** overfitting in machine learning models**.

[machine learning - Is overfitting "better" than underfitting? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/521835/is-overfitting-better-than-underfitting#:~:text=Overfitting%20is%20likely%20to%20be,neural%20network%20or%20polynomial%20model.)

**SVM is not very robust to outliers**. Presence of a few outliers can lead to very bad global misclassification.

- What is an advantage of using a PCA?

	- PCA can help us **improve performance at a very low cost of model accuracy**. Other benefits of PCA include reduction of noise in the data, feature selection (to a certain extent), and the ability to produce independent, uncorrelated features of the data.
- [r - How does centering make a difference in PCA (for SVD and eigen decomposition)? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/189822/how-does-centering-make-a-difference-in-pca-for-svd-and-eigen-decomposition)
- 
[How does centering the data get rid of the intercept in regression and PCA? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca%20)