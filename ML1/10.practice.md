-  [Why does the overfitting decreases if we choose K to be large in K-nearest neighbors?](https://datascience.stackexchange.com/questions/81866/why-does-the-overfitting-decreases-if-we-choose-k-to-be-large-in-k-nearest-neigh)

  
- What is Regularization in Machine Learning? Regularization refers to **techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting**.

- What is the difference between parameter and hyperparameter?

	- Model Parameters: These are the parameters in the model that must be determined using the training data set. These are the fitted parameters. **Hyperparameters: These are adjustable parameters that must be tuned in order to obtain a model with optimal performance**

-  What Is Grid Search?
	- Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on a the specific parameter values of a model. The model is also known as an estimator.Grid search exercise can save us time, effort and resources. 
-  Definite matrix 
	- https://en.wikipedia.org/wiki/Definite_matrix  
- graph of xˆ3
  -	![[Pasted image 20220712161951.png]]
-  C = 0 and C = infinite
	- **C hyperparameter** adds a penalty for each misclassified data point.
	- Large Value of parameter C implies a small margin, there is a tendency to overfit the training model.
	-  Small Value of parameter C implies a large margin which might lead to underfitting of the model.
	- [machine learning - What is the influence of C in SVMs with linear kernel? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel)

  

**Dimensionality reduction (DR) is another useful technique that can be used to mitigate/**less harsh** overfitting in machine learning models**.

[machine learning - Is overfitting "better" than underfitting? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/521835/is-overfitting-better-than-underfitting#:~:text=Overfitting%20is%20likely%20to%20be,neural%20network%20or%20polynomial%20model.)

**SVM is not very robust to outliers**. Presence of a few outliers can lead to very bad global misclassification.

- What is an advantage of using a PCA?

	- PCA can help us **improve performance at a very low cost of model accuracy**. Other benefits of PCA include reduction of noise in the data, feature selection (to a certain extent), and the ability to produce independent, uncorrelated features of the data.
- 
[How does centering the data get rid of the intercept in regression and PCA? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca%20)

- [Ridge Regression for Beginners! - YouTube](https://www.youtube.com/watch?v=OEU22e20tWw)

-  [machine learning - What is a channel in a CNN? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/64278/what-is-a-channel-in-a-cnn)
- An advantage of kernel ridge regression is that there exist formulas to compute the leave- one-out mean-squared error and classification error rate using the results of a single training on the whole training set, i.e. without actually performing the leave-one-out.

-  Which of the following advantages does ridge regression have over linear regression?
	-	Ridge **allows you to regularize ("shrink") coefficient estimates made by linear regression** (OLS). This means that the estimated coefficients are pushed towards 0, to make them work better on new data-sets ("optimized for prediction"). This allows you to use complex models and avoid over-fitting at the same time.
- [perceptron - Is there a proof to explain why XOR cannot be linearly separable? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/25228/is-there-a-proof-to-explain-why-xor-cannot-be-linearly-separable)
- [What is a filter in CNN? - Quora](https://www.quora.com/What-is-a-filter-in-CNN)
- [machine learning - What are the limitations of Kernel methods and when to use kernel methods? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/73944/what-are-the-limitations-of-kernel-methods-and-when-to-use-kernel-methods#:~:text=The%20complexity%20of%20kernel%20methods,this%20complexity%20is%20currently%20prohibitive.)
- [Why does L2 regularize increase the loss of a deep learning model? - Quora](https://www.quora.com/Why-does-L2-regularize-increase-the-loss-of-a-deep-learning-model)
- What is a feature map in SVM?

- But in SVM feature mapping, **the input will be mapped in a higher-dimensional feature space**, like in the above case, the basic function vector contains 5 values so it could be seen as mapping from 1-D input space to 4-D feature space (not including the bias-term ϕ0(x)).
- Kernel Trick
	- A Kernel Trick is **a simple method where a Non Linear data is projected onto a higher dimension space so as to make it easier to classify the data where it could be linearly divided by a plane**. This is mathematically achieved by Lagrangian formula using Lagrangian multipliers.
	- we computed a higher-dimensional inner product via a lower-dimensional one!
	- k (x, x ̃) := ⟨x, x ̃⟩2 is an example of a kernel.
	-  1  Formulate the (linear) learning machine (training and prediction) solely in terms of inner products
    -  2  Replace the inner products ⟨xi , xj ⟩ by the kernel k (xi , xj )
- [Scatter matrix , Covariance and Correlation Explained | by Raghavan | Medium](https://medium.com/@raghavan99o/scatter-matrix-covariance-and-correlation-explained-14921741ca56)

-  What Is Grid Search?
	- Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on a the specific parameter values of a model. The model is also known as an estimator.
- https://datascience.stackexchange.com/questions/81866/why-does-the-overfitting-decreases-if-we-choose-k-to-be-large-in-k-nearest-neigh
 - A function which is not bounded from above or below by a finite limit  is called an unbounded function
- Batch normalization
- <iframe width="480" height="240"  src="https://www.youtube.com/embed/DtEq44FTPM4" title="Batch Normalization - EXPLAINED!" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	- **Batch-Normalization (BN)** is an algorithmic method which makes the training of Deep Neural Networks (DNN) **faster** and **more stable**.It consists of **normalizing activation vectors from hidden layers** using the first and the second statistical moments (mean and variance) of the current batch. This normalization step is applied right before (or right after) the nonlinear function.
- Describe, with a formula, how the NCC classifies a new datapoint x based on c+ and c−.
	- ![[Pasted image 20220813113831.png]]
	- ![[Pasted image 20220814002241.png]]