[Random Forest Algorithm: A Complete Guide | Built In](https://builtin.com/data-science/random-forest-algorithm)

[Decision Trees in Machine Learning | by Prashant Gupta | Towards Data Science](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)

- Gini impurity measure
	- The Gini impurity measure is one of the methods used in decision tree algorithms to decide the optimal split from a root node, and subsequent splits.
	- Gini Impurity tells us what is the probability of misclassifying an observation
	- the lower the Gini the better the split. In other words the lower the likelihood of misclassification.
- ### Important Terminology related to Decision Trees

	1.  **Root Node:** It represents the entire population or sample and this further gets divided into two or more homogeneous sets.
	2.  **Splitting:** It is a process of dividing a node into two or more sub-nodes.
	3.  **Decision Node:** When a sub-node splits into further sub-nodes, then it is called the decision node.
	4.  **Leaf / Terminal Node:** Nodes do not split is called Leaf or Terminal node.
	5.  **Pruning:** When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting.
	6.  **Branch / Sub-Tree:** A subsection of the entire tree is called branch or sub-tree.
	7.  **Parent and Child Node:** A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node.